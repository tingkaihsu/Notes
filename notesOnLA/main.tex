\documentclass[12pt]{article}
\usepackage{braket}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{times}
\usepackage[export]{adjustbox}
\usepackage{listings}
\usepackage{mathcomp}
\usepackage{hyperref}
\usepackage{bm, amsmath}
\usepackage{float}
\usepackage{minted}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\title{Title}
\author{Ting-Kai Hsu}
\date{Date}

\begin{document}

\section{Transpose}

Transpose can be seen as the behavior of a linear system, that is to say, it contains the properties of linearity. The definition of transpose is as below,
\\
\indent For an arbitrary $n \times m$ matrix $A$\footnote{with $n$ rows and $m$ columns.}, the transpose of $A$ is,

\begin{center}
    \[ A \longrightarrow A^T = B \]
\end{center}

Where,

\begin{center}
    \[ a_{i, j} = b_{j, i}\ ,\ \ for\ i = 1, ..., n; j = 1, ..., m \]
\end{center}

By this definition, we see that $A^T = B$ is a $m \times n$ matrix, and simply put, \textbf{the rows become columns, and the columns become rows}.

\section{Matrix-Vector Product}

In this section, we will view the product of matrix and vector with two different aspects. First, given a $n \times m$ matrix $A$ and a vector $\mathbf{x}$ with $m$ rows, then their product can be expressed as,

\begin{center}
    \[ A\mathbf{x} = \begin{bmatrix}
        a_{11} & ... & a_{1m}\\
        . & ... & .\\
        . & ... & .\\
        . & ... & .\\
        a_{n1} & ... & a_{nm}\\
    \end{bmatrix} \begin{bmatrix}
        x_1\\
        .\\
        .\\
        .\\
        x_m\\
    \end{bmatrix} = \begin{bmatrix}
        a_{11}x_{1} + ... + a_{1m}x_{m}\\
        .\\.\\.\\
        a_{n1}x_{1} + ... + a_{nm}x_{m}\\
    \end{bmatrix}\]
\end{center}

In this point of view, we can say that we do the \textbf{inner product with the rows and vector}. On the other hand, 

\begin{center}
    \[ A\mathbf{x} = \begin{bmatrix}
        a_{11} & ... & a_{1m}\\
        . & ... & .\\
        . & ... & .\\
        . & ... & .\\
        a_{n1} & ... & a_{nm}\\
    \end{bmatrix} \begin{bmatrix}
        x_1\\
        .\\
        .\\
        .\\
        x_m\\
    \end{bmatrix} = x_{1}\begin{bmatrix}
        a_{11}\\
        .\\.\\.\\
        a_{1n}\\
    \end{bmatrix} + ... + x_{m}\begin{bmatrix}
        a_{1m}\\
        .\\.\\.\\
        a_{nm}\\
    \end{bmatrix} \]
\end{center}

This is the \textbf{weighted sums of columns}.
\\
\indent Moreover, we observe that the result of matrix-vector product is \textbf{another vector}, in this case, it is a vector \textbf{with n rows}. In engineer points of view, the matrix $A$ can represent the linear system, and the vector $\mathbf{x}$ can represent the input. After operating\footnote{in this case, timing the matrix with the vector input}, we get the vector output. In quantum mechanics, we view the operators as matrice, and by this means, we say that the physical state, that is, the ket, after operation(timing by matrix) is another physical state (ket).
\\
\indent Now a question arises, given two $m \times n$ martice $A$ and $B$, if $A\mathbf{w} = B \mathbf{w}$ for all vectors $\mathbf{w}$ in $\mathfrak{R}^n$, can we say the two matrice $A, B$ is the same?
The proof is simple, we make use of the concept of \textbf{standrad basic of $\mathfrak{R}^n$}, that is, ${\mathbf{e}_1, ..., \mathbf{e}_n}$. Therefore, we'll have,

\begin{center}
    \[ A\mathbf{e}_i = B\mathbf{e}_i,\ \ for\ i = 1, .. , n \]
    then,
    \[ A\mathbf{e}_i = \begin{bmatrix}
        a_{1i}\\
        .\\.\\.\\
        a_{ni}\\
    \end{bmatrix} \]
    \[ B\mathbf{e}_i = \begin{bmatrix}
        b_{1i}\\
        .\\.\\.\\
        b_{ni}\\
    \end{bmatrix} \]
\end{center}

Then we should be sure $A = B$.

\section{Eigenvalues and Eigenvectors}

Eigenvalues are useful when we want to find out the \textbf{powers of matrix}. First, we shall talk about what is \textbf{Eigenvectors}, it is origined from a concept that \textbf{almost every vector will change its direction after the matrix A acting on them.} However, there are some vectors that wouldn't change their direction, that is,

\begin{center}
    $A\mathbf{x} = \lambda \mathbf{x}$
\end{center}

Where $\lambda$ is a real number(or a comple number).
And we call $\lambda$ A's \textbf{eigenvalue} and \textbf{x} A's \textbf{eigenvector}.
\\
In \textbf{Dirac Notaion}, a vector $\mathbf{x}$ can been seen as a \textbf{ket}\footnote{However, there is a slightly difference between them, that is, the ket emphasizes on its direction but not its magnitude, yet for vector we consider both.} and the matirx $A$ can been seen as an \textbf{operator}. There Dirac notaion is as shown below:

\begin{center}
   $A\ket*{x} = \lambda \ket*{x}$ 
\end{center}

\section{The Way to Find Eigenvalues and Eigenvectors}

As we may see in the following secitons, it is worth mentioning the properties with eigenvalue:

\begin{center}
    \textit{m} by \textit{m} matrix should have \textbf{m} eigenvalues and \textbf{m} eigenvectors. 
\end{center}

As for the way to find out eigenvalue, we should use the following formula which would be fully explained in the following context:

\begin{center}
    $det(A-\lambda I) = 0$
\end{center}

\textbf{Example:} consider a 2 by 2 matrix A,
    \begin{center}
    $A=\begin{pmatrix}
        .8 & .3\\
        .2 & .7
    \end{pmatrix}$
\end{center}

and then we shall see $det(A-\lambda I) = 0$

\begin{center}
    $det\begin{pmatrix}
        .8-\lambda & .3\\
        .2 & .7-\lambda
        \end{pmatrix} = \lambda^{2}-1.5\lambda + 0.5 = (\lambda-1)(\lambda-\frac{1}{2})$
\end{center}

So the two eigenvalues are $\lambda = 1$ and $\lambda = \frac{1}{2}$.
For these eigenvalues, we can see that the matrix $A-\lambda I$ becomes \textit{singular matrix}\footnote{The definition of singular matrix is that the determination is 0, that is, not invertible}. And since the definition of eigenvector is as below:

\begin{center}
    $A \mathbf{x} = \lambda \mathbf{x}$
\end{center}

It is clearly\footnote{one can check this by writing out the system of equations} that

\begin{center}
    $(A-\lambda I)\mathbf{x} = \mathbf{0}$
\end{center}

So we can conclude that \textbf{eigenvectors} are in the \textbf{nullspace}\footnote{Nullspace is the space of solutions of $A\mathbf{x} = \mathbf{0}$, and is denoted by $\mathbf{N}(A)$. One can simply check that we are able to add adn multiply without leaving the nullspace, so it is a subsapce.} of $A- \lambda I$, that is, we'll have the following equation for the \textbf{example}:

\begin{center}
    $(A-I)\mathbf{x_{1} = \mathbf{0}}$
\end{center}

If we write it out explicitly,

\begin{center}
    $\begin{pmatrix}
        -0.2 & 0.3 \\
        0.2 & -0.3
    \end{pmatrix} \begin{pmatrix}
        x_1\\
        y_1
    \end{pmatrix}= \begin{pmatrix}
        0\\
        0
    \end{pmatrix}$
\end{center}

and then we'll have $0.2x_1 - 0.3y_1 = 0$, thus we can choose the eigenvector $\mathbf{x_{1}}$ to be $(.6, .4)$. Similarly, we'll have $\mathbf{x_{2}} = (.5, -.5)$.
Because of the definition of eigenvalue and eigenvector, we'll have the following properties:

\begin{center}
    \textit{When A is squared, the eigenvectors stay the same, but the eigenvalues are squared.}
\end{center}

The fact that \textbf{all other vectors are combinations of the eigenvectors}, and the proof needs to be specified. However, for now we shall use this properties and that \textbf{Each eigenvector is multiplied by its eigenvalue, when we multiply by A}, and we can accomplish all multiplication of all vectors with matrix A. After a short introdution to eigenvalue and eigenvector, one should know that \textit{special matrix will have special eigenvalue and eigenvector whose patterns and properties are worth studying}.

\section{Diagonalizing a Matrix}

From the above sections, we've already learnt the concept and definition of eigenvector and eigenvalue. In this section, we'll go through the most important application of them, that is, \textbf{Diagonalization}.

\begin{center}
    Suppose the \textit{n} by \textit{n} matrix A has \textit{n} linearly independent\footnote{without n independent eigenvectors, we can't diagonalize.} eigenvectors $\mathbf{x_1, ..., x_n}$. 
    \\
    Put them into the \textbf{columns} of an \textbf{eigenvector matrix} S. 
    \\
    Then we would have $S^{-1} A S$ is the \textbf{eigenvalue matrix} $\Lambda$ which is a diagonal matrix with \textbf{eigenvalue $\lambda$ on its diagonal}.
\end{center}

The proof is simple, consider A times S:

\begin{center}
    $AS = A \begin{pmatrix}
                \mathbf{x_1} & ... & \mathbf{x_n}
            \end{pmatrix} = \begin{pmatrix}
                \lambda_1\mathbf{x_1} & ... & \lambda_n\mathbf{x_n}
            \end{pmatrix}$
\end{center}

where we make use of the definition of eigenvalue and eigenvector.
and then the trick is to \textbf{split this matrix AS into S times $\Lambda$}.

\begin{center}
    $\begin{pmatrix}
        \lambda_1\mathbf{x_1} & ... & \lambda_n\mathbf{x_n}
    \end{pmatrix} = \begin{pmatrix}
        \mathbf{x_1} & ... & \mathbf{x_n}
    \end{pmatrix} \begin{pmatrix}
        \lambda_1 & 0 & ... & 0 & ...\\
        0 & \lambda_2 & ... & 0 & ...\\
        \ &\ & . \\
        \ &\ & \ & .\\
        \ &\ & \ & \ &.\\
        \ &\ & \ & \ & \ & \lambda_n\\
    \end{pmatrix} = S\Lambda$
\end{center}

then we get $AS = S\Lambda$ which implies $S^{-1}AS = \Lambda$.
\\
There are some remarks about $\Lambda$

\begin{itemize}
    \item \textbf{Remark 1} if the eigenvalues $\lambda_1, ..., \lambda_n$ are all different. Then it is automatic that the eigenvectors $\mathbf{x_1, ..., x_n}$ are independent.\footnote{The reason is that, if we try to express 0, for example, and consider only two eigenvalues $\lambda_1, \lambda_2$ and also two eigenvectors $\mathbf{x_1, x_2}$. And we have $c_1\mathbf{x_1}+c_2\mathbf{x_2} = \mathbf{0}$, \textbf{multiplied by A} and \textbf{multiplied by $\lambda_2$}, we'll get $c_1\lambda_1\mathbf{x_1} + c_2\lambda_2\mathbf{x_2} = \mathbf{0}$ and $c_1\lambda_2\mathbf{x_1} + c_2\lambda_2\mathbf{x_2} = \mathbf{0}$, and then we get $c_1 = 0, c_2 = 0$, which means there is only one way to express \textbf{0}.} And any matrix with no repeated eigenvalues can be diagonalized.
    \item \textbf{Remark 2} Some matrix with too few eigenvalues will make them undiagonalizable.
    \item \textbf{Remark 3} The eigenvectors are not unique, in other words, one can multiply some \textit{nonzero} constant.
\end{itemize}

\section{RREF}

RREF stands for \textit{Reduced Row Echelon Form}. Before introducing it, let's talk  about REF, row Echelon form. Here are the conditions,

\begin{itemize}
    \item Each \textit{nonzero} row lies above every \textit{zero} row.
    \item Leading entries are Echelon form, shown below, \[ \begin{bmatrix}
        1&0&0&6&0&3\\
        0&0&1&5&0&7\\
        0&0&0&2&0&4\\
        0&0&0&0&0&0\\
        0&0&0&0&0&0\\
    \end{bmatrix}\]
\end{itemize}

The leading entries are \textbf{the first nonzero number} in each row. Echelon form is that the leading entries in each nonzero row form like a stair goinh down from left to right. 
\\
\indent RREF inherits the properties of REF with additional condition, that is, the columns containing leading entries are columns standrad vector\footnote{\textbf{standrad vector} can be seen as unit vector.}. For example, 

\[ \begin{bmatrix}
    1&2&0&0&-1&-3\\
    0&0&1&0&0&-3\\
    0&0&0&1&1&2\\
    0&0&0&0&0&0\\
\end{bmatrix}
\]

As we can see, the 1st, 3rd, and 4th columns with leading entries are standard vectors. It can be proved that RREF is unique.
\end{document}